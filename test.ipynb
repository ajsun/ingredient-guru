{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from __future__ import print_function, unicode_literals\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = [\n",
    "      \"1 cup (2 sticks) butter, softened\",\n",
    "      \"3/4 cup granulated sugar\",\n",
    "      \"3/4 cup firmly packed brown sugar\",\n",
    "      \"2 eggs\",\n",
    "      \"1 tsp vanilla\",\n",
    "      \"2 1/4 cups flour\",\n",
    "      \"1 tsp baking soda\",\n",
    "      \"1/4  tsp salt\",\n",
    "      \"1  pkg (12 oz) BAKER'S Real Chocolate Chips\",\n",
    "      \"1 cup chopped nuts (optional)\",\n",
    "      \"1 lb Fettuccini\",\n",
    "      \"1 lb Shrimp, peeled and cleaned\",\n",
    "      \"4  Zucchini\",\n",
    "      \"1 lb Sliced mushrooms\",\n",
    "      \"8  Whole, fresh tomatoes\",\n",
    "      \"2 cloves Minced garlic\",\n",
    "      \"2 tsp Oregano\",\n",
    "      \"1 tsp Salt\",\n",
    "      \"1 tsp Pepper\",\n",
    "      \"1 tsp Virgin Olive Oil\",\n",
    "      \"1-1/2 lbs ground beef\",\n",
    "      \"1/2  cup A.1. Thick Hearty Steak Sauce\",\n",
    "      \"2 Tbs chopped oil-packed sun dried tomatoes\",\n",
    "      \"3 Tbs chopped fresh basil\",\n",
    "      \"1 Tbs toasted pine nuts\",\n",
    "      \"6  Sourdough rolls, split\",\n",
    "      \"3 oz goat cheese\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEASURE_WORDS = [\n",
    "    'tsp',\n",
    "    'tbs',\n",
    "    'tsps',\n",
    "    'pkg',\n",
    "    'cup',\n",
    "    'pint',\n",
    "    'quart',\n",
    "    'package',\n",
    "    'packet',\n",
    "    'gallon',\n",
    "    'ounce',\n",
    "    'oz',\n",
    "    'lb',\n",
    "    'cups',\n",
    "    'pints',\n",
    "    'quarts',\n",
    "    'gallons',\n",
    "    'liter',\n",
    "    'liters'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "butter NOUN ROOT butter\n",
      "Original: 1 cup (2 sticks) butter, softened\n",
      "-----Done-----\n",
      "sugar NOUN ROOT sugar\n",
      "Original: 3/4 cup granulated sugar\n",
      "-----Done-----\n",
      "sugar NOUN ROOT sugar\n",
      "Original: 3/4 cup firmly packed brown sugar\n",
      "-----Done-----\n",
      "eggs NOUN ROOT eggs\n",
      "Original: 2 eggs\n",
      "-----Done-----\n",
      "vanilla NOUN ROOT vanilla\n",
      "Original: 1 tsp vanilla\n",
      "-----Done-----\n",
      "Original: 2 1/4 cups flour\n",
      "-----Done-----\n",
      "baking VERB compound soda\n",
      "soda NOUN ROOT soda\n",
      "Original: 1 tsp baking soda\n",
      "-----Done-----\n",
      "salt NOUN ROOT salt\n",
      "Original: 1/4  tsp salt\n",
      "-----Done-----\n",
      "baker NOUN compound chips\n",
      "chocolate NOUN compound chips\n",
      "chips NOUN ROOT chips\n",
      "Original: 1  pkg (12 oz) BAKER'S Real Chocolate Chips\n",
      "-----Done-----\n",
      "Original: 1 cup chopped nuts (optional)\n",
      "-----Done-----\n",
      "fettuccini ADJ ROOT fettuccini\n",
      "Original: 1 lb Fettuccini\n",
      "-----Done-----\n",
      "shrimp NOUN ROOT shrimp\n",
      "Original: 1 lb Shrimp, peeled and cleaned\n",
      "-----Done-----\n",
      "zucchini NOUN ROOT zucchini\n",
      "Original: 4  Zucchini\n",
      "-----Done-----\n",
      "mushrooms NOUN ROOT mushrooms\n",
      "Original: 1 lb Sliced mushrooms\n",
      "-----Done-----\n",
      "tomatoes NOUN ROOT tomatoes\n",
      "Original: 8  Whole, fresh tomatoes\n",
      "-----Done-----\n",
      "cloves NOUN nsubj garlic\n",
      "garlic VERB ROOT garlic\n",
      "Original: 2 cloves Minced garlic\n",
      "-----Done-----\n",
      "oregano NOUN ROOT oregano\n",
      "Original: 2 tsp Oregano\n",
      "-----Done-----\n",
      "salt NOUN ROOT salt\n",
      "Original: 1 tsp Salt\n",
      "-----Done-----\n",
      "pepper NOUN ROOT pepper\n",
      "Original: 1 tsp Pepper\n",
      "-----Done-----\n",
      "olive ADJ compound oil\n",
      "oil NOUN ROOT oil\n",
      "Original: 1 tsp Virgin Olive Oil\n",
      "-----Done-----\n",
      "ground NOUN compound beef\n",
      "beef NOUN ROOT beef\n",
      "Original: 1-1/2 lbs ground beef\n",
      "-----Done-----\n",
      "steak NOUN compound sauce\n",
      "sauce NOUN ROOT sauce\n",
      "Original: 1/2  cup A.1. Thick Hearty Steak Sauce\n",
      "-----Done-----\n",
      "oil NOUN compound sun\n",
      "sun NOUN compound tomatoes\n",
      "tomatoes VERB ROOT tomatoes\n",
      "Original: 2 Tbs chopped oil-packed sun dried tomatoes\n",
      "-----Done-----\n",
      "basil NOUN ROOT basil\n",
      "Original: 3 Tbs chopped fresh basil\n",
      "-----Done-----\n",
      "pine NOUN compound nuts\n",
      "nuts NOUN ROOT nuts\n",
      "Original: 1 Tbs toasted pine nuts\n",
      "-----Done-----\n",
      "sourdough NOUN compound rolls\n",
      "rolls NOUN ROOT rolls\n",
      "Original: 6  Sourdough rolls, split\n",
      "-----Done-----\n",
      "goat NOUN compound cheese\n",
      "cheese NOUN ROOT cheese\n",
      "Original: 3 oz goat cheese\n",
      "-----Done-----\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def clean_text(text):\n",
    "    # removing any text in parenthesis\n",
    "    text = re.sub(r'\\(.+\\)', '', text)\n",
    "    # convert to lower case\n",
    "    text = text.lower()\n",
    "    text = text.split(' ')\n",
    "    while '' in text:\n",
    "        text.remove('')\n",
    "    while \" \" in text:\n",
    "        text.remove(\" \")\n",
    "    while \"\\n\" in text:\n",
    "        text.remove(\"\\n\")\n",
    "    while \"\\n\\n\" in text:\n",
    "        text.remove(\"\\n\\n\")\n",
    "    return u' '.join(text)\n",
    "\n",
    "def find_and_store_digit(text):\n",
    "    '''\n",
    "    :text - text to take digit\n",
    "    :return: - returns digit\n",
    "    '''\n",
    "    digits = re.findall(r'\\d*.*\\d', text)\n",
    "    \n",
    "    if len(digits) > 0:\n",
    "        return digits[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def remove_digits(text):\n",
    "    return re.sub(r'\\d*.*\\d', '', text)\n",
    "\n",
    "\n",
    "for t in texts:\n",
    "#     print(t)\n",
    "    clean_text = cleanText(t)\n",
    "    digit = find_and_store_digit(clean_text)\n",
    "    clean_text = remove_digits(clean_text)\n",
    "    \n",
    "    doc = nlp(clean_text)\n",
    "    desc = []\n",
    "    core = []\n",
    "    measure = []\n",
    "    for word in doc:\n",
    "        if word.pos_ == 'ADJ' or word.pos_ == 'VERB':\n",
    "            desc.append(word.text)\n",
    "        elif word.pos_ in ['ADV', 'SPACE', 'PUNCT', 'PART', 'CCONJ']:\n",
    "            continue\n",
    "        elif word.text in MEASURE_WORDS:\n",
    "            measure.append(word.text)\n",
    "        else:\n",
    "            core.append(word.text)\n",
    "    basic_text = u' '.join(core)\n",
    "    # new parser    \n",
    "    new_doc = nlp(basic_text)\n",
    "    for word in new_doc:\n",
    "        print(word.text, word.pos_, word.dep_, word.head)\n",
    "    print(\"Original: %s\" % t)\n",
    "#     print(\"Cleaned: %s\" % u' '.join(core))\n",
    "    print(\"-----Done-----\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://nicschrading.com/project/Intro-to-NLP-with-spaCy/\n",
    "https://explosion.ai/blog/chatbot-node-js-spacy\n",
    "https://github.com/explosion/spaCy/blob/master/examples/training/train_new_entity_type.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.metrics import accuracy_score\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A custom function to tokenize the text using spaCy\n",
    "# and convert to lemmas\n",
    "def tokenizeText(sample):\n",
    "\n",
    "    # get the tokens using spaCy\n",
    "    tokens = nlp(sample)\n",
    "\n",
    "    # lemmatize\n",
    "    lemmas = []\n",
    "    for tok in tokens:\n",
    "        lemmas.append(tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_)\n",
    "    tokens = lemmas\n",
    "\n",
    "    # stoplist the tokens\n",
    "#     tokens = [tok for tok in tokens if tok not in STOPLIST]\n",
    "\n",
    "    # stoplist symbols\n",
    "#     tokens = [tok for tok in tokens if tok not in SYMBOLS]\n",
    "\n",
    "    # remove large strings of whitespace\n",
    "    while \"\" in tokens:\n",
    "        tokens.remove(\"\")\n",
    "    while \" \" in tokens:\n",
    "        tokens.remove(\" \")\n",
    "    while \"\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\")\n",
    "    while \"\\n\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\\n\")\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def printNMostInformative(vectorizer, clf, N):\n",
    "    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    topClass1 = coefs_with_fns[:N]\n",
    "    topClass2 = coefs_with_fns[:-(N + 1):-1]\n",
    "    print(\"Class 1 best: \")\n",
    "    for feat in topClass1:\n",
    "        print(feat)\n",
    "    print(\"Class 2 best: \")\n",
    "    for feat in topClass2:\n",
    "        print(feat)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "    ...,\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a classifier: a support vector classifier\n",
    "classifier = SVC(gamma=0.001)\n",
    "vectorizer = CountVectorizer(tokenizer=tokenizeText, ngram_range=(1,1))\n",
    "# the pipeline to clean, tokenize, vectorize, and classify\n",
    "pipe = Pipeline([('vectorizer', vectorizer), ('clf', classifier)])\n",
    "\n",
    "\n",
    "train = [\"1 oz chicken\", \"1 oz beef\", \"1 cup of mixed vegetables\", \"8 apples\", \"Two slices of chicken\", \"1/4 lb beef\", \"2 carrots\"]\n",
    "\n",
    "labelsTrain = [\"meat/poultry\", \"meat/poultry\", \"vegetables\", \"fruits\", \"meat/poultry\", \"meat/poultry\", \"vegetables\"]\n",
    "\n",
    "pipe.fit(train, labelsTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------\n",
      "results:\n",
      "1 cup of mixed vegetables : meat/poultry\n",
      "accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "test = [\"1 cup of mixed vegetables\"]\n",
    "labelsTest = [\"vegetables\"]\n",
    "\n",
    "preds = pipe.predict(test)\n",
    "print(\"----------------------------------------------------------------------------------------------\")\n",
    "print(\"results:\")\n",
    "for (sample, pred) in zip(test, preds):\n",
    "    print(sample, \":\", pred)\n",
    "print(\"accuracy:\", accuracy_score(labelsTest, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
